{
    "cells": [
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from IPython.display import Markdown, display\ndef printmd(string):\n    display(Markdown('# <span style=\"color:blue\">'+string+'</span>'))\n\nif('sc' in locals() or 'sc' in globals()):\n    printmd(\"Error: This is Watson Studio Apache. Change is needed.\")\nelse:\n    printmd(\"All is good. Let's go.\")",
            "execution_count": 1,
            "outputs": [
                {
                    "output_type": "display_data",
                    "data": {
                        "text/plain": "<IPython.core.display.Markdown object>",
                        "text/markdown": "# <span style=\"color:blue\">All is good. Let's go.</span>"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "!pip install pyspark==2.4.5",
            "execution_count": 2,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Requirement already satisfied: pyspark==2.4.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (2.4.5)\r\nRequirement already satisfied: py4j==0.10.7 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pyspark==2.4.5) (0.10.7)\r\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession",
            "execution_count": 3,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()",
            "execution_count": 4,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "!wget https://github.com/IBM/coursera/raw/master/coursera_ml/a2.parquet",
            "execution_count": 5,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "--2020-12-23 20:21:37--  https://github.com/IBM/coursera/raw/master/coursera_ml/a2.parquet\nResolving github.com (github.com)... 140.82.113.4\nConnecting to github.com (github.com)|140.82.113.4|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://github.com/IBM/skillsnetwork/raw/master/coursera_ml/a2.parquet [following]\n--2020-12-23 20:21:38--  https://github.com/IBM/skillsnetwork/raw/master/coursera_ml/a2.parquet\nReusing existing connection to github.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/IBM/skillsnetwork/master/coursera_ml/a2.parquet [following]\n--2020-12-23 20:21:38--  https://raw.githubusercontent.com/IBM/skillsnetwork/master/coursera_ml/a2.parquet\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 59032 (58K) [application/octet-stream]\nSaving to: \u2018a2.parquet.4\u2019\n\n100%[======================================>] 59,032      --.-K/s   in 0.001s  \n\n2020-12-23 20:21:38 (45.6 MB/s) - \u2018a2.parquet.4\u2019 saved [59032/59032]\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Now it\u2019s time to have a look at the recorded sensor data. You should see data similar to the one exemplified below\u2026."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "df = spark.read.parquet('a2.parquet')\ndf.createOrReplaceTempView('df')\n\ndf_two_classes = spark.sql(\"SELECT * FROM df\")\nspark.sql(\"SELECT * FROM df\").show()",
            "execution_count": 6,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "+-----+-----------+-------------------+-------------------+-------------------+\n|CLASS|   SENSORID|                  X|                  Y|                  Z|\n+-----+-----------+-------------------+-------------------+-------------------+\n|    0|         26| 380.66434005495194| -139.3470983812975|-247.93697521077704|\n|    0|         29| 104.74324299209692| -32.27421440203938|-25.105013725863852|\n|    0| 8589934658| 118.11469236129976| 45.916682927433534| -87.97203782706572|\n|    0|34359738398| 246.55394030642543|-0.6122810693132044|-398.18662513951506|\n|    0|17179869241|-190.32584900181487|  234.7849657520335|-206.34483804019288|\n|    0|25769803830| 178.62396382387422| -47.07529438881511|  84.38310769821979|\n|    0|25769803831|  85.03128805189493|-4.3024316644854546|-1.1841857567516714|\n|    0|34359738411| 26.786262674736566| -46.33193951911338| 20.880756008396055|\n|    0| 8589934592|-16.203752396859194| 51.080957032176954| -96.80526656416971|\n|    0|25769803852|   47.2048142440404|  -78.2950899652916| 181.99604091494786|\n|    0|34359738369| 15.608872398939273| -79.90322809181754|  69.62150711098005|\n|    0|         19|-4.8281721129789315| -67.38050508399905| 221.24876396496404|\n|    0|         54| -98.40725712852762|-19.989364074314732|  -302.695196085276|\n|    0|17179869313| 22.835845394816594|   17.1633660118843| 32.877914832011385|\n|    0|34359738454|  84.20178070080324| -32.81572075916947| -48.63517643958031|\n|    0|          0|  56.54732521345129| -7.980106018032676|  95.05162719436447|\n|    0|17179869201|  -57.6008655247749|  5.135393798773895| 236.99158698947267|\n|    0|17179869308| -65.59264738389012| -48.92660057215126| -61.58970715383383|\n|    0|25769803790|  34.82337351291005|  9.483542084393937|  197.6066372962772|\n|    0|25769803825|  39.80573823439121|-0.7955236412785212| -79.66652640650325|\n+-----+-----------+-------------------+-------------------+-------------------+\nonly showing top 20 rows\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import random\nrandom.seed(34)\nsplits = df_two_classes.randomSplit([0.6,0.2,0.2])\ndf_train = splits[0]\ndf_valid = splits[1]\ndf_test = splits[2]",
            "execution_count": 7,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Please create a VectorAssembler which consumes columns X, Y and Z and produces a column \u201cfeatures\u201d"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark.ml.feature import VectorAssembler\nassembler = VectorAssembler(inputCols =['X','Y','Z'], outputCol = \"features\")",
            "execution_count": 8,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Please instantiate a classifier from the SparkML package and assign it to the classifier variable. Make sure to either: \n\nRename the \u201cCLASS\u201d column to \u201clabel\u201d \n\nor \n\nSpecify the label-column correctly to be \u201cCLASS\u201d"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark.ml.classification import GBTClassifier\n\nclassifier = GBTClassifier(labelCol='CLASS',featuresCol='features',maxIter=10)",
            "execution_count": 9,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "\nLet\u2019s train and evaluate\u2026"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark.ml import Pipeline\npipeline = Pipeline(stages=[assembler, classifier])",
            "execution_count": 10,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "model = pipeline.fit(df_train)",
            "execution_count": 11,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "prediction = model.transform(df_train)",
            "execution_count": 12,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\nbinEval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\").setPredictionCol(\"prediction\").setLabelCol(\"CLASS\")\n    \nbinEval.evaluate(prediction)",
            "execution_count": 13,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 13,
                    "data": {
                        "text/plain": "0.9989136338946225"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "model = pipeline.fit(df_valid)\nprediction = model.transform(df_valid)\nbinEval.evaluate(prediction)",
            "execution_count": 14,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 14,
                    "data": {
                        "text/plain": "0.9983277591973244"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "prediction = model.transform(df_test)\nbinEval.evaluate(prediction)",
            "execution_count": 15,
            "outputs": [
                {
                    "output_type": "execute_result",
                    "execution_count": 15,
                    "data": {
                        "text/plain": "0.9933665008291874"
                    },
                    "metadata": {}
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "!rm -Rf a2_m2.json",
            "execution_count": 16,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "prediction = prediction.repartition(1)\nprediction.write.json('a2_m2.json')",
            "execution_count": 17,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "!rm -f rklib.py\n!wget https://raw.githubusercontent.com/IBM/coursera/master/rklib.py",
            "execution_count": 18,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "--2020-12-23 20:22:57--  https://raw.githubusercontent.com/IBM/coursera/master/rklib.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2540 (2.5K) [text/plain]\nSaving to: \u2018rklib.py\u2019\n\n100%[======================================>] 2,540       --.-K/s   in 0s      \n\n2020-12-23 20:22:57 (18.9 MB/s) - \u2018rklib.py\u2019 saved [2540/2540]\n\n",
                    "name": "stdout"
                }
            ]
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "import zipfile\nimport os\n\ndef zipdir(path, ziph):\n    for root, dirs, files in os.walk(path):\n        for file in files:\n            ziph.write(os.path.join(root, file))\n\nzipf = zipfile.ZipFile('a2_m2.json.zip', 'w', zipfile.ZIP_DEFLATED)\nzipdir('a2_m2.json', zipf)\nzipf.close()",
            "execution_count": 19,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "!base64 a2_m2.json.zip > a2_m2.json.zip.base64",
            "execution_count": 20,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "from rklib import submit\nkey = \"J3sDL2J8EeiaXhILFWw2-g\"\npart = \"G4P6f\"\nemail = 'jonah.winninghoff@gmail.com'\ntoken = 'bUwiKYcD1OpsjQAe'\nwith open('a2_m2.json.zip.base64', 'r') as myfile:\n    data=myfile.read()\nsubmit(email, token, key, part, [part], data)",
            "execution_count": 21,
            "outputs": [
                {
                    "output_type": "stream",
                    "text": "Submission successful, please check on the coursera grader page for the status\n-------------------------\n{\"elements\":[{\"itemId\":\"LTL4F\",\"id\":\"f_F-qCtuEei_fRLwaVDk3g~LTL4F~pp7hNUVcEeuHJA4YeLGq-w\",\"courseId\":\"f_F-qCtuEei_fRLwaVDk3g\"}],\"paging\":{},\"linked\":{}}\n-------------------------\n",
                    "name": "stdout"
                }
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}
